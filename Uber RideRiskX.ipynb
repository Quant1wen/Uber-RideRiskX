{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:51.075889Z",
     "start_time": "2025-08-25T02:47:51.070287Z"
    }
   },
   "source": [
    "# 0) Setup (Python 3.9)\n",
    "# If needed, install:\n",
    "# !pip install pandas numpy scikit-learn xgboost plotly matplotlib seaborn joblib shap xlrd openpyxl streamlit\n",
    "\n",
    "import os, json, math, textwrap, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import streamlit as st\n",
    "from datetime import datetime, time as dtime\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss,\n",
    "    confusion_matrix, precision_recall_curve, roc_curve\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import xgboost as xgb\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:51.107957Z",
     "start_time": "2025-08-25T02:47:51.102658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Paths & Config\n",
    "DATA_PATH = \"ncr_ride_bookings.csv\"   # change if needed\n",
    "ARTIFACTS_DIR = \"artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "LABEL_COL = \"is_cancelled\"\n",
    "DATE_COL = \"Date\"\n",
    "TIME_COL = \"Time\"\n",
    "\n",
    "# Columns present in your dataset (for reference)\n",
    "RAW_COLS = [\n",
    "    \"Date\",\"Time\",\"Booking ID\",\"Booking Status\",\"Customer ID\",\"Vehicle Type\",\n",
    "    \"Pickup Location\",\"Drop Location\",\"Avg VTAT\",\"Avg CTAT\",\n",
    "    \"Cancelled Rides by Customer\",\"Reason for cancelling by Customer\",\n",
    "    \"Cancelled Rides by Driver\",\"Driver Cancellation Reason\",\n",
    "    \"Incomplete Rides\",\"Incomplete Rides Reason\",\n",
    "    \"Booking Value\",\"Ride Distance\",\"Driver Ratings\",\"Customer Rating\",\"Payment Method\"\n",
    "]\n",
    "\n",
    "# Columns that leak the outcome or occur after the event — DROP from features\n",
    "LEAKY_COLS = [\n",
    "    \"Booking Status\",                    # becomes our label\n",
    "    \"Cancelled Rides by Customer\",\n",
    "    \"Reason for cancelling by Customer\",\n",
    "    \"Cancelled Rides by Driver\",\n",
    "    \"Driver Cancellation Reason\",\n",
    "    \"Incomplete Rides\",\n",
    "    \"Incomplete Rides Reason\",\n",
    "    \"Booking ID\",                        # identifier (not predictive by itself)\n",
    "]\n",
    "\n",
    "# Cost settings for threshold search (you can tune)\n",
    "COST_TP = +5.0   # benefit of catching a would-be cancellation\n",
    "COST_FP = -1.0   # cost of unnecessary intervention\n"
   ],
   "id": "5d3563c12fef5a8f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:51.641060Z",
     "start_time": "2025-08-25T02:47:51.119713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Load & quick browse\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(df_raw.head(3))\n",
    "print(\"\\nNull counts:\\n\", df_raw.isna().sum())\n",
    "print(\"\\nDtypes:\\n\", df_raw.dtypes)"
   ],
   "id": "7c6540d6e32493d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (150000, 21)\n",
      "         Date      Time    Booking ID   Booking Status   Customer ID  \\\n",
      "0  2024-03-23  12:29:38  \"CNR5884300\"  No Driver Found  \"CID1982111\"   \n",
      "1  2024-11-29  18:01:39  \"CNR1326809\"       Incomplete  \"CID4604802\"   \n",
      "2  2024-08-23  08:56:10  \"CNR8494506\"        Completed  \"CID9202816\"   \n",
      "\n",
      "  Vehicle Type Pickup Location      Drop Location  Avg VTAT  Avg CTAT  ...  \\\n",
      "0        eBike     Palam Vihar            Jhilmil       NaN       NaN  ...   \n",
      "1     Go Sedan   Shastri Nagar  Gurgaon Sector 56       4.9      14.0  ...   \n",
      "2         Auto         Khandsa      Malviya Nagar      13.4      25.8  ...   \n",
      "\n",
      "   Reason for cancelling by Customer Cancelled Rides by Driver  \\\n",
      "0                                NaN                       NaN   \n",
      "1                                NaN                       NaN   \n",
      "2                                NaN                       NaN   \n",
      "\n",
      "   Driver Cancellation Reason Incomplete Rides  Incomplete Rides Reason  \\\n",
      "0                         NaN              NaN                      NaN   \n",
      "1                         NaN              1.0        Vehicle Breakdown   \n",
      "2                         NaN              NaN                      NaN   \n",
      "\n",
      "  Booking Value  Ride Distance  Driver Ratings  Customer Rating  \\\n",
      "0           NaN            NaN             NaN              NaN   \n",
      "1         237.0           5.73             NaN              NaN   \n",
      "2         627.0          13.58             4.9              4.9   \n",
      "\n",
      "   Payment Method  \n",
      "0             NaN  \n",
      "1             UPI  \n",
      "2      Debit Card  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Null counts:\n",
      " Date                                      0\n",
      "Time                                      0\n",
      "Booking ID                                0\n",
      "Booking Status                            0\n",
      "Customer ID                               0\n",
      "Vehicle Type                              0\n",
      "Pickup Location                           0\n",
      "Drop Location                             0\n",
      "Avg VTAT                              10500\n",
      "Avg CTAT                              48000\n",
      "Cancelled Rides by Customer          139500\n",
      "Reason for cancelling by Customer    139500\n",
      "Cancelled Rides by Driver            123000\n",
      "Driver Cancellation Reason           123000\n",
      "Incomplete Rides                     141000\n",
      "Incomplete Rides Reason              141000\n",
      "Booking Value                         48000\n",
      "Ride Distance                         48000\n",
      "Driver Ratings                        57000\n",
      "Customer Rating                       57000\n",
      "Payment Method                        48000\n",
      "dtype: int64\n",
      "\n",
      "Dtypes:\n",
      " Date                                  object\n",
      "Time                                  object\n",
      "Booking ID                            object\n",
      "Booking Status                        object\n",
      "Customer ID                           object\n",
      "Vehicle Type                          object\n",
      "Pickup Location                       object\n",
      "Drop Location                         object\n",
      "Avg VTAT                             float64\n",
      "Avg CTAT                             float64\n",
      "Cancelled Rides by Customer          float64\n",
      "Reason for cancelling by Customer     object\n",
      "Cancelled Rides by Driver            float64\n",
      "Driver Cancellation Reason            object\n",
      "Incomplete Rides                     float64\n",
      "Incomplete Rides Reason               object\n",
      "Booking Value                        float64\n",
      "Ride Distance                        float64\n",
      "Driver Ratings                       float64\n",
      "Customer Rating                      float64\n",
      "Payment Method                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:54.586674Z",
     "start_time": "2025-08-25T02:47:51.663106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Label + Booking-time feature engineering (no leakage)\n",
    "\n",
    "def _parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    # Tries multiple formats robustly\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def _parse_time_series(s: pd.Series) -> pd.Series:\n",
    "    # If \"HH:MM:SS\" or \"HH:MM\" – otherwise coerce\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.time\n",
    "\n",
    "def add_booking_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Label: cancellation if Booking Status contains 'Cancelled'\n",
    "    out[LABEL_COL] = out[\"Booking Status\"].astype(str).str.contains(\"cancel\", case=False, na=False).astype(int)\n",
    "\n",
    "    # Parse date/time\n",
    "    out[\"_Date_dt\"] = _parse_date_series(out[DATE_COL])\n",
    "    # Time may be a string or datetime-like\n",
    "    tparsed = pd.to_datetime(out[TIME_COL], errors=\"coerce\")\n",
    "    out[\"_hour\"] = tparsed.dt.hour\n",
    "\n",
    "    # is_weekend (Sat=5, Sun=6)\n",
    "    out[\"is_weekend\"] = out[\"_Date_dt\"].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "    # is_late: 00:00–05:59\n",
    "    out[\"is_late\"] = out[\"_hour\"].between(0, 5, inclusive=\"both\").astype(int)\n",
    "\n",
    "    # Basic distance and fare safety features\n",
    "    out[\"fare_per_km\"] = np.where(out[\"Ride Distance\"].fillna(0) > 0,\n",
    "                                  out[\"Booking Value\"] / out[\"Ride Distance\"].replace(0, np.nan),\n",
    "                                  np.nan)\n",
    "\n",
    "    # Simple location relationship\n",
    "    out[\"same_area\"] = (out[\"Pickup Location\"].astype(str).str.lower()\n",
    "                        == out[\"Drop Location\"].astype(str).str.lower()).astype(int)\n",
    "\n",
    "    # Keep only booking-time-safe features (drop leakage)\n",
    "    drop_cols = set(LEAKY_COLS)\n",
    "    out = out.drop(columns=[c for c in out.columns if c in drop_cols], errors=\"ignore\")\n",
    "\n",
    "    return out\n",
    "\n",
    "df = add_booking_time_features(df_raw)\n",
    "\n",
    "print(\"Engineered shape:\", df.shape)\n",
    "print(df[[DATE_COL, TIME_COL, \"is_weekend\", \"is_late\", \"fare_per_km\", \"same_area\", LABEL_COL]].head(5))"
   ],
   "id": "24397aea5c791b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered shape: (150000, 20)\n",
      "         Date      Time  is_weekend  is_late  fare_per_km  same_area  \\\n",
      "0  2024-03-23  12:29:38           1        0          NaN          0   \n",
      "1  2024-11-29  18:01:39           0        0    41.361257          0   \n",
      "2  2024-08-23  08:56:10           0        0    46.170839          0   \n",
      "3  2024-10-21  17:17:25           0        0    12.228101          0   \n",
      "4  2024-09-16  22:08:00           0        0    15.287285          0   \n",
      "\n",
      "   is_cancelled  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:54.716463Z",
     "start_time": "2025-08-25T02:47:54.608984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Train/Val/Test split by time (chronological quantiles)\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, date_col: str = \"_Date_dt\",\n",
    "                     train_q: float = 0.7, val_q: float = 0.85) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    d = df.copy()\n",
    "    d = d.sort_values(date_col)\n",
    "    # If some dates are NaT, push them to the beginning to avoid leakage\n",
    "    d[date_col] = d[date_col].fillna(d[date_col].min())\n",
    "    cut1 = d[date_col].quantile(train_q)\n",
    "    cut2 = d[date_col].quantile(val_q)\n",
    "    tr = d[d[date_col] <= cut1]\n",
    "    va = d[(d[date_col] > cut1) & (d[date_col] <= cut2)]\n",
    "    te = d[d[date_col] > cut2]\n",
    "    return tr, va, te\n",
    "\n",
    "train_df, val_df, test_df = time_based_split(df)\n",
    "\n",
    "for name, part in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    print(name, part.shape, \"pos rate=\", part[LABEL_COL].mean().round(4))\n"
   ],
   "id": "28d01e073326888d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (105340, 20) pos rate= 0.2509\n",
      "val (22517, 20) pos rate= 0.245\n",
      "test (22143, 20) pos rate= 0.251\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:54.731907Z",
     "start_time": "2025-08-25T02:47:54.727760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5) Columns: numeric & categorical\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"Avg VTAT\", \"Avg CTAT\", \"Booking Value\", \"Ride Distance\",\n",
    "    \"Driver Ratings\", \"Customer Rating\", \"fare_per_km\", \"is_weekend\", \"is_late\", \"same_area\", \"_hour\"\n",
    "]\n",
    "CAT_COLS = [\n",
    "    \"Customer ID\", \"Vehicle Type\", \"Pickup Location\", \"Drop Location\", \"Payment Method\"\n",
    "]\n",
    "\n",
    "# Keep only cols that actually exist\n",
    "NUM_COLS = [c for c in NUM_COLS if c in train_df.columns]\n",
    "CAT_COLS = [c for c in CAT_COLS if c in train_df.columns]\n",
    "\n",
    "TARGET = LABEL_COL\n",
    "FEATURE_COLS = NUM_COLS + CAT_COLS\n"
   ],
   "id": "f9a3fb6caa5e690c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:54.794036Z",
     "start_time": "2025-08-25T02:47:54.788234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6) Helper: reduce high-cardinality categories to Top-K + \"Other\"\n",
    "from collections import Counter\n",
    "\n",
    "class TopKCategorical(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_k: int = 30):\n",
    "        self.top_k = top_k\n",
    "        self.keep_maps_: Dict[str, set] = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.keep_maps_ = {}\n",
    "        for col in X.columns:\n",
    "            vc = Counter(X[col].astype(str).fillna(\"nan\"))\n",
    "            keep = set([c for c, _ in vc.most_common(self.top_k)])\n",
    "            self.keep_maps_[col] = keep\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            keep = self.keep_maps_.get(col, set())\n",
    "            X[col] = X[col].astype(str).where(X[col].astype(str).isin(keep), other=\"Other\")\n",
    "        return X\n"
   ],
   "id": "8326862854e878f3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:54.837833Z",
     "start_time": "2025-08-25T02:47:54.831042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7) Build preprocessing + model pipeline\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"topk\", TopKCategorical(top_k=40)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, NUM_COLS),\n",
    "        (\"cat\", cat_pipe, CAT_COLS),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Compute class imbalance to set scale_pos_weight\n",
    "pos_rate = train_df[TARGET].mean()\n",
    "neg_rate = 1 - pos_rate\n",
    "scale_pos_weight = (neg_rate / pos_rate) if pos_rate > 0 else 1.0\n",
    "print(\"Estimated scale_pos_weight:\", round(scale_pos_weight, 3))\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=2,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=SEED,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    early_stopping_rounds= 50\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"xgb\", xgb_clf)\n",
    "])"
   ],
   "id": "cab7df51516b66d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated scale_pos_weight: 2.986\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:56.747212Z",
     "start_time": "2025-08-25T02:47:54.909600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8) Train with early stopping on validation\n",
    "\n",
    "X_tr, y_tr = train_df[FEATURE_COLS], train_df[TARGET]\n",
    "X_va, y_va = val_df[FEATURE_COLS], val_df[TARGET]\n",
    "\n",
    "# To use early stopping, we temporarily fit preprocess, transform arrays, then fit XGB directly\n",
    "X_tr_enc = preprocess.fit_transform(X_tr, y_tr)\n",
    "X_va_enc = preprocess.transform(X_va)\n",
    "\n",
    "xgb_clf.fit(\n",
    "    X_tr_enc, y_tr,\n",
    "    eval_set=[(X_va_enc, y_va)],\n",
    "    verbose=50,\n",
    ")\n",
    "\n",
    "# Wrap the fitted preprocess + fitted booster back into a single pipeline-like dict\n",
    "trained = {\n",
    "    \"preprocess\": preprocess,\n",
    "    \"booster\": xgb_clf\n",
    "}\n",
    "\n",
    "# Quick validation metrics (uncalibrated)\n",
    "va_pred = xgb_clf.predict_proba(X_va_enc)[:, 1]\n",
    "print(\"Val AUC:\", roc_auc_score(y_va, va_pred))\n",
    "print(\"Val PR-AUC:\", average_precision_score(y_va, va_pred))\n",
    "print(\"Val Brier:\", brier_score_loss(y_va, va_pred))\n",
    "print(\"Val LogLoss:\", log_loss(y_va, va_pred))\n"
   ],
   "id": "17ca2054d83e20c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.99944\n",
      "[50]\tvalidation_0-auc:0.99944\n",
      "[51]\tvalidation_0-auc:0.99944\n",
      "Val AUC: 0.9994477763916878\n",
      "Val PR-AUC: 0.997722921691302\n",
      "Val Brier: 0.18518442975339489\n",
      "Val LogLoss: 0.5626819036451683\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:56.957362Z",
     "start_time": "2025-08-25T02:47:56.845106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9) Probability calibration (Isotonic via CalibratedClassifierCV with prefit model)\n",
    "# Calibrating often improves probability quality for banding and cost-based decisions.\n",
    "\n",
    "calibrator = CalibratedClassifierCV(\n",
    "    estimator=xgb_clf, method=\"isotonic\", cv=\"prefit\"\n",
    ")\n",
    "calibrator.fit(X_va_enc, y_va)\n",
    "\n",
    "# Store a thin wrapper to keep consistent API with preprocessing\n",
    "class CalibratedModel:\n",
    "    def __init__(self, preprocess, calibrator):\n",
    "        self.preprocess = preprocess\n",
    "        self.calibrator = calibrator\n",
    "    def predict_proba(self, X):\n",
    "        Xenc = self.preprocess.transform(X)\n",
    "        return self.calibrator.predict_proba(Xenc)\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        p = self.predict_proba(X)[:, 1]\n",
    "        return (p >= threshold).astype(int)\n",
    "\n",
    "calibrated_model = CalibratedModel(preprocess, calibrator)\n",
    "\n",
    "# Check calibrated val metrics\n",
    "va_pred_cal = calibrated_model.predict_proba(X_va)[:, 1]\n",
    "print(\"Val AUC (cal):\", roc_auc_score(y_va, va_pred_cal))\n",
    "print(\"Val PR-AUC (cal):\", average_precision_score(y_va, va_pred_cal))\n",
    "print(\"Val Brier (cal):\", brier_score_loss(y_va, va_pred_cal))\n",
    "print(\"Val LogLoss (cal):\", log_loss(y_va, va_pred_cal))\n"
   ],
   "id": "db59d90e569f6dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC (cal): 0.9994716917762212\n",
      "Val PR-AUC (cal): 0.9975615423941916\n",
      "Val Brier (cal): 0.002768512272185855\n",
      "Val LogLoss (cal): 0.012122059277611723\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:57.228777Z",
     "start_time": "2025-08-25T02:47:57.069068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 10) Risk threshold (Low/High) via cost-sensitive search on validation\n",
    "# Interpretations:\n",
    "#  - Predict High (intervene) when p >= t_cut\n",
    "#  - Utility = +COST_TP for each true cancel flagged High,  -COST_FP for each non-cancel flagged High\n",
    "\n",
    "def utility_two_band(y_true, p, t_cut, cost_tp=COST_TP, cost_fp=COST_FP):\n",
    "    y = np.asarray(y_true); p = np.asarray(p)\n",
    "    H = p >= t_cut\n",
    "    # High + true cancel -> reward; High + non-cancel -> penalty\n",
    "    util = np.sum((y == 1) & H) * cost_tp + np.sum((y == 0) & H) * cost_fp\n",
    "    return float(util)\n",
    "\n",
    "def search_single_threshold(y_true, p, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 181)  # 0.05 to 0.95 step 0.005\n",
    "    best = (-1e18, 0.5)\n",
    "    for t in grid:\n",
    "        u = utility_two_band(y_true, p, t)\n",
    "        if u > best[0]:\n",
    "            best = (u, float(t))\n",
    "    return {\"utility\": best[0], \"t_cut\": best[1]}\n",
    "\n",
    "thr = search_single_threshold(y_va, va_pred_cal)  # use calibrated probs if you kept calibration\n",
    "thr\n",
    "\n"
   ],
   "id": "caff510c578de01d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utility': np.float64(27260.0), 't_low': 0.1, 't_high': 0.5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:57.389249Z",
     "start_time": "2025-08-25T02:47:57.309128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 11) Final evaluation on TEST with the calibrated model and found cutoff\n",
    "X_te, y_te = test_df[FEATURE_COLS], test_df[TARGET]\n",
    "te_pred_cal = calibrated_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "def summarize_two_band_perf(y_true, p, t_cut):\n",
    "    H = (p >= t_cut).astype(int)     # High label\n",
    "    L = (p < t_cut).astype(int)      # Low label\n",
    "    auc = roc_auc_score(y_true, p)\n",
    "    ap = average_precision_score(y_true, p)\n",
    "    brier = brier_score_loss(y_true, p)\n",
    "    lg = log_loss(y_true, p)\n",
    "    return {\n",
    "        \"AUC\": round(auc, 4), \"PR_AUC\": round(ap, 4),\n",
    "        \"Brier\": round(brier, 4), \"LogLoss\": round(lg, 4),\n",
    "        \"Share_High\": round(H.mean(), 4),\n",
    "        \"Share_Low\": round(L.mean(), 4),\n",
    "        \"Utility\": round(utility_two_band(y_true, p, t_cut), 2)\n",
    "    }\n",
    "\n",
    "test_summary = summarize_two_band_perf(y_te, te_pred_cal, thr[\"t_cut\"])\n",
    "test_summary\n",
    "\n"
   ],
   "id": "4c419f3afabfeae3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.9997,\n",
       " 'PR_AUC': 0.9986,\n",
       " 'Brier': 0.002,\n",
       " 'LogLoss': 0.0091,\n",
       " 'Share_High': np.float64(0.249),\n",
       " 'Share_Med': np.float64(0.0),\n",
       " 'Share_Low': np.float64(0.751),\n",
       " 'Utility': np.float64(27564.75)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:57.982539Z",
     "start_time": "2025-08-25T02:47:57.493928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 12) Plots (ROC, PR, Calibration curve) – saved to artifacts\n",
    "\n",
    "def plot_roc_pr(y_true, p, tag=\"val\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, p)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, p)\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "    plt.plot([0,1], [0,1], ls=\"--\")\n",
    "    plt.title(f\"ROC ({tag})\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ARTIFACTS_DIR, f\"roc_{tag}.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(rec, prec, lw=2)\n",
    "    plt.title(f\"Precision-Recall ({tag})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ARTIFACTS_DIR, f\"pr_{tag}.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "plot_roc_pr(y_va, va_pred_cal, \"val\")\n",
    "plot_roc_pr(y_te, te_pred_cal, \"test\")\n",
    "print(\"Saved ROC/PR plots to\", ARTIFACTS_DIR)\n"
   ],
   "id": "e614fb3bf8769f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC/PR plots to artifacts\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:58.082552Z",
     "start_time": "2025-08-25T02:47:58.068024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 15) Save artifacts for deployment (two-band)\n",
    "joblib.dump(trained[\"preprocess\"], os.path.join(ARTIFACTS_DIR, \"preprocess.joblib\"))\n",
    "joblib.dump(trained[\"booster\"], os.path.join(ARTIFACTS_DIR, \"xgb_model.joblib\"))\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"risk_thresholds.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"t_cut\": thr[\"t_cut\"]}, f, indent=2)\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"model_card.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"feature_columns\": FEATURE_COLS,\n",
    "        \"label_column\": LABEL_COL,\n",
    "        \"metrics_test\": test_summary\n",
    "    }, f, indent=2)\n",
    "\n"
   ],
   "id": "9b2015621c3d4ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved under: artifacts\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:58.227508Z",
     "start_time": "2025-08-25T02:47:58.219356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 16) Batch scoring for Excel files: read xlsx/csv → output scored file with RiskBand\n",
    "# Usage: score_file(\"new_orders.xlsx\")\n",
    "\n",
    "def load_artifacts():\n",
    "    prep = joblib.load(os.path.join(ARTIFACTS_DIR, \"preprocess.joblib\"))\n",
    "    booster = joblib.load(os.path.join(ARTIFACTS_DIR, \"xgb_model.joblib\"))\n",
    "    with open(os.path.join(ARTIFACTS_DIR, \"risk_thresholds.json\"), encoding=\"utf-8\") as f:\n",
    "        thr = json.load(f)  # expects {\"t_cut\": ...}\n",
    "    class _Wrap:\n",
    "        def __init__(self, preprocess, booster):\n",
    "            self.preprocess = preprocess\n",
    "            self.booster = booster\n",
    "        def predict_proba(self, X):\n",
    "            Xenc = self.preprocess.transform(X)\n",
    "            return self.booster.predict_proba(Xenc)\n",
    "    return _Wrap(prep, booster), thr\n",
    "\n",
    "def classify_two_band(p: np.ndarray, t_cut: float) -> list:\n",
    "    return [\"High\" if prob >= t_cut else \"Low\" for prob in p]\n",
    "\n",
    "def score_dataframe(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    model_like, thr = load_artifacts()\n",
    "    dfx = add_booking_time_features(df_in)\n",
    "    X = dfx[FEATURE_COLS]\n",
    "    p = model_like.predict_proba(X)[:, 1]\n",
    "    out = df_in.copy()\n",
    "    out[\"CancelRisk_Prob\"] = p\n",
    "    out[\"CancelRisk_Band\"] = classify_two_band(p, thr[\"t_cut\"])\n",
    "    return out\n",
    "\n",
    "def score_file(path_in: str, path_out: str = None):\n",
    "    ext = os.path.splitext(path_in)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        df_in = pd.read_excel(path_in)\n",
    "    elif ext in [\".csv\"]:\n",
    "        df_in = pd.read_csv(path_in)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input format; use .xlsx or .csv\")\n",
    "\n",
    "    out = score_dataframe(df_in)\n",
    "    if not path_out:\n",
    "        base = os.path.splitext(path_in)[0]\n",
    "        path_out = base + \"_scored.xlsx\"\n",
    "    out.to_excel(path_out, index=False)\n",
    "    print(\"Saved:\", path_out)\n",
    "\n",
    "# Example:\n",
    "# score_file(\"incoming_orders.xlsx\")\n"
   ],
   "id": "637ad105c378a387",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:58.338379Z",
     "start_time": "2025-08-25T02:47:58.331212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 18) Lightweight Streamlit dashboard (save as app.py and run: streamlit run app.py)\n",
    "STREAMLIT_APP = r'''\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import streamlit as st\n",
    "\n",
    "ARTIFACTS_DIR = \"artifacts\"\n",
    "DATA_PATH = \"/mnt/data/ncr_ride_bookings.csv\"  # change as needed\n",
    "\n",
    "st.set_page_config(page_title=\"Uber Ride Cancellation Risk\", layout=\"wide\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_artifacts():\n",
    "    prep = joblib.load(os.path.join(ARTIFACTS_DIR, \"preprocess.joblib\"))\n",
    "    booster = joblib.load(os.path.join(ARTIFACTS_DIR, \"xgb_model.joblib\"))\n",
    "    with open(os.path.join(ARTIFACTS_DIR, \"risk_thresholds.json\")) as f:\n",
    "        thr = json.load(f)\n",
    "    return prep, booster, thr\n",
    "\n",
    "def add_booking_time_features(df):\n",
    "    import pandas as pd, numpy as np\n",
    "    from datetime import datetime\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"is_cancelled\"] = out[\"Booking Status\"].astype(str).str.contains(\"cancel\", case=False, na=False).astype(int)\n",
    "    out[\"_Date_dt\"] = pd.to_datetime(out[\"Date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "    tparsed = pd.to_datetime(out[\"Time\"], errors=\"coerce\")\n",
    "    out[\"_hour\"] = tparsed.dt.hour\n",
    "    out[\"is_weekend\"] = out[\"_Date_dt\"].dt.weekday.isin([5,6]).astype(int)\n",
    "    out[\"is_late\"] = out[\"_hour\"].between(0,5, inclusive=\"both\").astype(int)\n",
    "    out[\"fare_per_km\"] = np.where(out[\"Ride Distance\"].fillna(0) > 0,\n",
    "                                  out[\"Booking Value\"] / out[\"Ride Distance\"].replace(0, np.nan),\n",
    "                                  np.nan)\n",
    "    out[\"same_area\"] = (out[\"Pickup Location\"].astype(str).str.lower()\n",
    "                        == out[\"Drop Location\"].astype(str).str.lower()).astype(int)\n",
    "    DROP = set([\n",
    "        \"Booking Status\",\"Cancelled Rides by Customer\",\"Reason for cancelling by Customer\",\n",
    "        \"Cancelled Rides by Driver\",\"Driver Cancellation Reason\",\"Incomplete Rides\",\n",
    "        \"Incomplete Rides Reason\",\"Booking ID\"\n",
    "    ])\n",
    "    out = out.drop(columns=[c for c in out.columns if c in DROP], errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "NUM_COLS = [\"Avg VTAT\",\"Avg CTAT\",\"Booking Value\",\"Ride Distance\",\n",
    "            \"Driver Ratings\",\"Customer Rating\",\"fare_per_km\",\"is_weekend\",\"is_late\",\"same_area\",\"_hour\"]\n",
    "CAT_COLS = [\"Customer ID\",\"Vehicle Type\",\"Pickup Location\",\"Drop Location\",\"Payment Method\"]\n",
    "FEATURE_COLS = NUM_COLS + CAT_COLS\n",
    "\n",
    "st.title(\"Uber Ride Cancellation Risk – Driver Plugin Prototype\")\n",
    "\n",
    "uploaded = st.file_uploader(\"Upload Excel/CSV with upcoming bookings\", type=[\"xlsx\",\"xls\",\"csv\"])\n",
    "prep, booster, thr = load_artifacts()\n",
    "\n",
    "def predict_proba(df_in):\n",
    "    dfx = add_booking_time_features(df_in)\n",
    "    X = dfx[FEATURE_COLS]\n",
    "    Xenc = prep.transform(X)\n",
    "    p = booster.predict_proba(Xenc)[:,1]\n",
    "    return p\n",
    "\n",
    "def band(p):\n",
    "    if p >= thr[\"t_high\"]:\n",
    "        return \"High\"\n",
    "    elif p >= thr[\"t_low\"]:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "if uploaded is not None:\n",
    "    ext = os.path.splitext(uploaded.name)[1].lower()\n",
    "    if ext in [\".xlsx\",\".xls\"]:\n",
    "        df_in = pd.read_excel(uploaded)\n",
    "    else:\n",
    "        df_in = pd.read_csv(uploaded)\n",
    "\n",
    "    p = predict_proba(df_in)\n",
    "    df_out = df_in.copy()\n",
    "    df_out[\"CancelRisk_Prob\"] = p\n",
    "    df_out[\"CancelRisk_Band\"] = [band(v) for v in p]\n",
    "\n",
    "    st.subheader(\"Scored Orders\")\n",
    "    st.dataframe(df_out.head(50))\n",
    "\n",
    "    st.download_button(\"Download Scored File\", df_out.to_csv(index=False).encode(), file_name=\"scored_orders.csv\")\n",
    "\n",
    "    st.subheader(\"Risk Distribution\")\n",
    "    fig = px.histogram(df_out, x=\"CancelRisk_Prob\", nbins=30, color=\"CancelRisk_Band\")\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "else:\n",
    "    st.info(\"Upload an Excel/CSV file to score cancellation risk.\")\n",
    "'''\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"app.py\"), \"w\") as f:\n",
    "    f.write(STREAMLIT_APP)\n",
    "print(\"Streamlit app saved to artifacts/app.py (run: streamlit run artifacts/app.py)\")"
   ],
   "id": "2f4ecbeaf3e53ecb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit app saved to artifacts/app.py (run: streamlit run artifacts/app.py)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:47:58.437088Z",
     "start_time": "2025-08-25T02:47:58.433312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = [os.path.join(ARTIFACTS_DIR, f) for f in os.listdir(ARTIFACTS_DIR)]\n",
    "print(files)"
   ],
   "id": "ff59c47d6fd8ade5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artifacts\\\\app.py', 'artifacts\\\\model_card.json', 'artifacts\\\\preprocess.joblib', 'artifacts\\\\pr_test.png', 'artifacts\\\\pr_val.png', 'artifacts\\\\risk_thresholds.json', 'artifacts\\\\roc_test.png', 'artifacts\\\\roc_val.png', 'artifacts\\\\xgb_model.joblib']\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
